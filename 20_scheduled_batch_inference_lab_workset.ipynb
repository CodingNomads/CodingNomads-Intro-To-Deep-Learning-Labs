{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ESEkRyOvXNp7"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install libraries\n",
    "!apt install -qq cron\n",
    "!pip install -Uqq fastai transformers accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oKI1rZrJz25s"
   },
   "outputs": [],
   "source": [
    "# Write the directory we will store the CSVs\n",
    "!mkdir csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5T5FOBjavzyx",
    "outputId": "2b79fe66-c9f5-4812-da7b-cda7f6cc7bd6"
   },
   "outputs": [],
   "source": [
    "%%writefile scrape_tweets.py\n",
    "#!/usr/bin/env python\n",
    "from transformers import pipeline\n",
    "from random import shuffle\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Download and/or load gpt-2 for text generation\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", device_map=\"auto\")\n",
    "\n",
    "# Generate positive tweets starting with \"I love my iPhone \"\n",
    "positive = generator(\"I love my iPhone \", max_length=40, num_return_sequences=5)\n",
    "# Generate negative tweets starting with \"I don't like my iPhone \"\n",
    "negative = generator(\"I don't like my iPhone \", max_length=40, num_return_sequences=5)\n",
    "# Combine the two lists into a dataframe\n",
    "df = pd.DataFrame(positive + negative).rename(columns={\"generated_text\": \"tweet\"})\n",
    "# Create timestamps with some randomness, then add them to the df\n",
    "df[\"timestamp\"] = [datetime.now() + timedelta(seconds=np.random.random() * 60) for _ in range(len(df))]\n",
    "# Sort the dataframe by timestamp and reset the index\n",
    "df.sort_values(\"timestamp\", inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# Save to CSV\n",
    "df.to_csv(f\"/content/csvs/en_apple-{datetime.now()}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sndEHC-gY-v-",
    "outputId": "92147edd-3b38-4f23-e1f1-5762a2bf1552"
   },
   "outputs": [],
   "source": [
    "# Start crontab\n",
    "!service cron start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cr1cryiGy1CO",
    "outputId": "7375f58a-eceb-4ee6-af18-fb2d77c6e28b"
   },
   "outputs": [],
   "source": [
    "%%writefile crontab.e\n",
    "* * * * * python3 /content/scrape_tweets.py >> /content/cron.log 2>&1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ETxFwwgezIhX"
   },
   "outputs": [],
   "source": [
    "!crontab crontab.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zS7s2Kfz-av",
    "outputId": "81130218-1226-43e4-f1be-78e79fe263c6"
   },
   "outputs": [],
   "source": [
    "!crontab -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCOaQhJ2sLXS",
    "outputId": "b28d4a6f-4928-4d1b-dcac-ec340259bf62"
   },
   "outputs": [],
   "source": [
    "!sleep 120\n",
    "!ls csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ihj49vrnSciD",
    "outputId": "81cbe61d-4b53-4769-f408-5bdaae864b1d"
   },
   "outputs": [],
   "source": [
    "!tail -n 10 /content/cron.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndvFecDOdHmQ"
   },
   "source": [
    "# Exercise 20.1\n",
    "\n",
    "### Train and save your classifier here\n",
    "\n",
    "Use this section to train a RNN to classify text by sentiment using a dataset of your choice.\n",
    "Alternately, if you feel like you have a good handle on training, feel free to use one of the many [pre-trained sentiment analysis models on huggingface](https://huggingface.co/models?search=sentiment).\n",
    "We don't particularly care about the predictive performance since accuracy isn't the goal of this lesson, but it should perform better than random chance, just for sanity.\n",
    "\n",
    "<!-- startquestion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F-UvB1_1bRez"
   },
   "outputs": [],
   "source": [
    "# Create datasets/dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "H9ZvgGxkPdSe"
   },
   "outputs": [],
   "source": [
    "# Train or fine-tune a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qLHHs2uJPe9t"
   },
   "outputs": [],
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YLdwotspIFM"
   },
   "source": [
    "# Exercise 20.2\n",
    "\n",
    "### Write your inference Python file in the cell below\n",
    "\n",
    "The cell below writes out a python file that should be executed every minute after tweets are collected.\n",
    "This script should:\n",
    "\n",
    "* Load the traine model from Exercise 20.1\n",
    "* Load the most recent CSV of tweets into a DataFrame\n",
    "* Make inferences on the tweets in the data from the previous step, storing results in a `\"sentiment\"` column\n",
    "* Count the negative and positive tweets for the log\n",
    "* Write the DataFrame with the additional column back to the original CSV\n",
    "\n",
    "Once this .py file is complete, we will update our crontab so that this script runs every minute right after tweets are scraped.\n",
    "\n",
    "<!-- startquestion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pOnDMNdpMxN",
    "outputId": "2d071410-ddde-4a9a-c920-a4ce1000aadb"
   },
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "# Imports\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads and returns a trained model.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def load_most_recent_csv():\n",
    "    \"\"\"\n",
    "    Loads and returns the most recent CSV of tweet data for inference,\n",
    "    and the path to that CSV.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def add_sentiment_inference_column(data, model):\n",
    "    \"\"\"\n",
    "    Uses a model inferences to add a 'sentiment' column to the data\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def count_neg_pos_tweets(data):\n",
    "    \"\"\"\n",
    "    Returns the number of negative and number of positive tweets\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def write_results(data, path):\n",
    "    \"\"\"\n",
    "    Overwrites the CSV file from the dataframe\n",
    "    that includes the inference results.\n",
    "    \"\"\"\n",
    "\n",
    "def main():\n",
    "    # Load the trained model\n",
    "    print(f'{datetime.now()}: Loading model...')\n",
    "    model = load_model()\n",
    "\n",
    "    # Load the most recent CSV\n",
    "    print(f'{datetime.now()}: Loading CSV...')\n",
    "    data, path = load_most_recent_csv()\n",
    "\n",
    "    # Write predictions to a new column called \"sentiment\"\n",
    "    print(f'{datetime.now()}: Making inferences...')\n",
    "    data_with_inferences = add_sentiment_inference_column(data, model)\n",
    "\n",
    "    # Print a statement with the number of negative tweets found for our logs\n",
    "    n_negative_tweets, n_positive_tweets = count_neg_pos_tweets(data_with_inferences)\n",
    "    print(f'{datetime.now()}: We recorded {n_negative_tweets} negative tweets out of {n_total_tweets} tweets ')\n",
    "\n",
    "    # Write out the CSV to back to the original file\n",
    "    write_results(data, path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhawZETLqt7k",
    "outputId": "0dac080b-4877-4ace-e2e1-defba64d0c73"
   },
   "outputs": [],
   "source": [
    "%%writefile crontab.e\n",
    "* * * * * python3 /content/scrape_tweets.py >> /content/cron.log 2>&1\n",
    "* * * * * python3 /content/inference.py >> /content/inference.log 2>&1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Vwq-tP30Qc3Q"
   },
   "outputs": [],
   "source": [
    "!crontab crontab.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6bya5oorkcp",
    "outputId": "6d24251a-2ba9-4139-c353-2ff39bc88bce"
   },
   "outputs": [],
   "source": [
    "!crontab -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPlLdWkIyXla"
   },
   "source": [
    "# Exercise 20.3\n",
    "\n",
    "In this exercise, you will check the outputs of at least one CSV to understand if it's working as expected.\n",
    "In the cells below, please:\n",
    "\n",
    "* use the `!head` and `!tail` bash commands to inspect the most recent CSV. Does it contain the `sentiment` column?\n",
    "* load the most recent CSV with `pandas`. Take a look at a few of the inferences. Did your model perform OK in a qualitative sense? Note that we shouldn't expect perfect performance if the training set wasn't tweets.\n",
    "\n",
    "<!-- startquestion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KZCZOd74yntB"
   },
   "outputs": [],
   "source": [
    "# Your work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ahrb4hiPPCiD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "20_scheduled_batch_inference_lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
