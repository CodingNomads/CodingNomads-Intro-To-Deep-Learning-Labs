{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UVZlK6AjphyK"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Zhrag2TVqUut"
   },
   "outputs": [],
   "source": [
    "def seed_all(seed=42):\n",
    "    \"\"\"\n",
    "    Sets the numpy and torch random seed.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.random.seed = seed\n",
    "\n",
    "seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3i5JbcrJp8Ng"
   },
   "outputs": [],
   "source": [
    "# Create some X data\n",
    "X = np.random.uniform(0, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uWXuFmJPqglE"
   },
   "outputs": [],
   "source": [
    "# Define the slope (m), bias (b), and some noise we want to add to X to make y\n",
    "m = 3\n",
    "b = 1.8\n",
    "noise = np.random.normal(scale=3, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaATIViIcyK_"
   },
   "source": [
    "# Exercise 2.1\n",
    "\n",
    "Working with `np.Array` and `torch.Tensor` objects are very similar!\n",
    "This exercise is intended as a warm-up for array operations that will be common throughout the course.\n",
    "Based on your knowledge of linear regression, please use the variables `X`, `m`, `b`, and `noise` to generate a new variable `y`.\n",
    "The output should be the target variable `y`, where `X` and `y` are related in a linear fashion, but with some noise.\n",
    "\n",
    "<!-- startquestion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cCGl3ng2dSRw"
   },
   "outputs": [],
   "source": [
    "y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "id": "6R0i1APRq0SW",
    "outputId": "87a07812-c1bb-4c79-f4e8-66fcff6d2fe1"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('y')\n",
    "ax.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mMYN-sAdqc1"
   },
   "source": [
    "# Exercise 2.2\n",
    "\n",
    "Now that you've learned about the Mean Squared Error (MSE) loss function, it's time to put your knowledge into practice. In this exercise, you'll be tasked with implementing the MSE loss function in Python.\n",
    "\n",
    "Write a Python function named `mse` that takes two lists of equal length as input: `ys` (the actual values) and `yhats` (the predicted values). Your function should return the Mean Squared Error between the actual and predicted values.\n",
    "\n",
    "We've provided some python code to get you started.\n",
    "\n",
    "<!-- startquestion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "duBXRwuidjI0"
   },
   "outputs": [],
   "source": [
    "# Define MSE\n",
    "def mse(predictions:torch.Tensor, actuals:torch.Tensor) -> torch.Tensor:\n",
    "    # your code here\n",
    "    raise NotImplementedError(\"Implement MSE, then remove this line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YXVJjYjQYS-r"
   },
   "outputs": [],
   "source": [
    "ys = torch.tensor([1,2,3])\n",
    "yhats = torch.tensor([1.1, 2.1, 3.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aqjXsI8dr4ba"
   },
   "outputs": [],
   "source": [
    "assert torch.allclose(mse(ys, yhats),  torch.tensor(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXN3G-Fq4m9a",
    "outputId": "3eb799b6-743b-4595-98c0-bc19c783f059"
   },
   "outputs": [],
   "source": [
    "print(mse(ys, yhats))\n",
    "print(mse(ys, yhats).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmtAEb1_YOo6",
    "outputId": "30f891f3-1377-4daa-d512-1f37c4128997"
   },
   "outputs": [],
   "source": [
    "# Now that we've defined MSE, let's just use Torch's.\n",
    "mse_loss = nn.MSELoss()\n",
    "mse_loss(ys, yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HROGoKWjY0g1",
    "outputId": "d2d88bd5-81e8-4df4-fcf6-59b43665398e"
   },
   "outputs": [],
   "source": [
    "# We can also use the functional API to calculate MSE\n",
    "F.mse_loss(ys, yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_i03QSwHZQFj"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yIFPBzglI7u"
   },
   "source": [
    "# Exercise 2.3\n",
    "\n",
    "In the exercise below, your task is to fit a `LinearRegression` model using `scikit-learn`.\n",
    "This involves training the model on our dataset and then inspecting the learned parameters - the coefficient (slope) and intercept.\n",
    "You'll also calculate the mean squared error (MSE) of the model's predictions, which gives us a measure of how well the model fits the data.\n",
    "\n",
    "<!-- startquestion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "M1rMNqDneidD"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "lr = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lp5rB1BWZnyE",
    "outputId": "b6c4d840-fffc-49e4-bead-f5463213f7d4"
   },
   "outputs": [],
   "source": [
    "# Display the slope and intercept\n",
    "lr.coef_, lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PAk2Xt3cAEy",
    "outputId": "b446cd58-4bae-4bd0-c7d9-44686bb5654a"
   },
   "outputs": [],
   "source": [
    "# Calculate the mean squared error\n",
    "mean_squared_error(y, lr.predict(X.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "id": "8eIhs0pjZyem",
    "outputId": "ff830a13-f0a9-4262-cf23-5d6b25eb5ada"
   },
   "outputs": [],
   "source": [
    "# Plot our line of best fit\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('y')\n",
    "ax.scatter(X, y)\n",
    "_x = np.arange(0, 10)\n",
    "_y = _x * lr.coef_[0] + lr.intercept_\n",
    "ax.plot(_x, _y, c='red', label=f\"Line of best fit\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zOJ2WiAyxGyo"
   },
   "outputs": [],
   "source": [
    "# Because we're in torch now, let's just turn X and y into tensors.\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5Kd4qcgnnK3"
   },
   "source": [
    "# Exercise 2.4\n",
    "\n",
    "In the exercise below, your task is to complete the `forward` method within the `LinReg` class. This method should define how the input `X` is transformed to produce the output for a linear regression model.\n",
    "\n",
    "Before we dive into the exercise, let's understand what `nn.Parameter` does. In PyTorch, `nn.Parameter` is a special kind of Tensor that automatically registers itself as a parameter when assigned as an attribute to a `Module`. This is particularly useful when we want certain tensors to be considered as trainable parameters of our model. In our case, the slope and the bias are the parameters that the model will learn during training.\n",
    "\n",
    "<!-- startquestion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-2dfhB3fg1Jn"
   },
   "outputs": [],
   "source": [
    "# Build our linear regression model\n",
    "class LinReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Randomly initialize 2 parameters, one for our slope and one for our bias.\n",
    "        self.slope = nn.Parameter(torch.rand(1))\n",
    "        self.bias = nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "okgeYEnUtfHx"
   },
   "outputs": [],
   "source": [
    "lr = LinReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "g5peJppY0Tsm"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 300\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGi-JIk-0ZP7",
    "outputId": "1c0560e8-1e89-4444-d68e-6f4c6a059356"
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store slopes, biases, losses, and alphas for visualization later\n",
    "slopes = []\n",
    "biases = []\n",
    "losses = []\n",
    "_alphas = []\n",
    "\n",
    "# Loop over the number of epochs\n",
    "for i in range(N_EPOCHS):\n",
    "    # Generate predictions using the current model\n",
    "    yhat = lr(X)\n",
    "\n",
    "    # Compute the loss between the predictions and actual values\n",
    "    loss = F.mse_loss(yhat, y)\n",
    "\n",
    "    # Print the loss every 10% of the total epochs\n",
    "    if i % (N_EPOCHS / 10) == 0:\n",
    "        print(f\"Epoch {i} Train Loss: {loss:.04f}\")\n",
    "\n",
    "    # Compute the gradients of the loss with respect to the model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the model parameters using the computed gradients and the learning rate\n",
    "    lr.slope.data.sub_(lr.slope.grad * LR)\n",
    "    lr.bias.data.sub_(lr.bias.grad * LR)\n",
    "\n",
    "    # Reset the gradients to zero for the next iteration\n",
    "    lr.slope.grad.zero_()\n",
    "    lr.bias.grad.zero_()\n",
    "\n",
    "    # Store the current parameters and loss for visualization later\n",
    "    slopes.append(float(lr.slope.data.detach().numpy()))\n",
    "    biases.append(float(lr.bias.data.detach().numpy()))\n",
    "    losses.append(float(loss.detach().numpy()))\n",
    "    _alphas.append(i / N_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wgyFeEBudKpF",
    "outputId": "df6a39c7-230a-4b02-de2f-1d11c6f2326d"
   },
   "outputs": [],
   "source": [
    "lr.slope, lr.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "id": "qrWoCKIV1z4X",
    "outputId": "10554543-6dd5-4d59-e3ec-8a49a218d863"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('y')\n",
    "ax.scatter(X, y)\n",
    "for s, b, a in zip(slopes, biases, _alphas):\n",
    "    _x = np.arange(0, 10)\n",
    "    _y = _x * s + b\n",
    "    ax.plot(_x, _y, alpha=a, c='red', label=f\"Epoch {int(a)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "7gQXK8bw2NL-",
    "outputId": "5e8fba84-6ad9-4b36-812e-0ae9c8e2febb"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "ax.plot(losses)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (MSE)')\n",
    "if (losses[-1] > losses[0]) | np.isnan(losses[-1]):\n",
    "    ax.set_title('Diverging - BAD!')\n",
    "else:\n",
    "    ax.set_title('Converging - goood!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMMiW5YRhCQF"
   },
   "source": [
    "# Exercise 2.5\n",
    "\n",
    "Now that you've seen how the training loop works, it's time for you to experiment with different hyperparameters. Specifically, we'll be adjusting the learning rate (`LR`) and the number of epochs (`N_EPOCHS`).\n",
    "\n",
    "Hyperparameters are settings that we can tune to control how the model learns. They are not learned from the data but are set prior to training. The learning rate and the number of epochs are two of the most important hyperparameters in gradient descent.\n",
    "\n",
    "1. **Learning Rate (`LR`):** This controls the size of the steps that we take during gradient descent. A larger learning rate means we take bigger steps, while a smaller learning rate means we take smaller steps.\n",
    "\n",
    "2. **Number of Epochs (`N_EPOCHS`):** This is simply the number of times the training loop is run. More epochs mean more opportunities for the model to learn from the data.\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "1. Change the values of `LR` and `N_EPOCHS` in the code.\n",
    "2. Re-run the code up to this point. Remember to re-instantiate your model every time to start from scratch.\n",
    "3. Observe the changes in the model's performance.\n",
    "\n",
    "Things to consider:\n",
    "\n",
    "- What happens if you make the learning rate too large? What about if it's too small?\n",
    "- How does changing the number of epochs affect the model's performance and the number of epochs required to reach the loss minimum?\n",
    "- Can you find a combination of `LR` and `N_EPOCHS` that gives you the best performance in the fewest number of epochs?\n",
    "\n",
    "Remember, machine learning involves a lot of experimentation. Don't be afraid to try different values and see what happens. Happy tuning!\n",
    "\n",
    "<!-- startquestion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1BMXex0Xjlty"
   },
   "outputs": [],
   "source": [
    "# Go back and re-run the code with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-VVJBDwN6X-_"
   },
   "outputs": [],
   "source": [
    "# Make yet another fake dataset\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=1000, n_features=3, n_informative=2, bias=3, noise=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "3XHTBONL8APT"
   },
   "outputs": [],
   "source": [
    "# No more bad habits, we need to split our data.\n",
    "X_train, X_valid, y_train, y_valid = (torch.tensor(i).float() for i in train_test_split(X, y, test_size=0.1, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9o7gG1hqaey"
   },
   "source": [
    "# Exercise 2.6\n",
    "\n",
    "In the following exercise, your task is to create a few `nn.Parameter`s that will represent our weights and bias for the multi-variable linear regression model. Here's what you need to do:\n",
    "\n",
    "1. **Weights Parameter:** Create a `weights` parameter that matches the number of columns in `X_train`. Each weight corresponds to a feature in our input data. Initialize these weights with random numbers, which you can generate using `torch.rand`.\n",
    "\n",
    "2. **Bias Parameter:** Create a `bias` parameter that represents the bias term in our regression model. This should be a single value, also initialized with a random number using `torch.rand`.\n",
    "\n",
    "Remember, these parameters (weights and bias) are the values that our model will learn to adjust during the training process to minimize the loss function. Initially, we set them to random values, but as the model learns from the data, these parameters will be updated to better fit our data.\n",
    "\n",
    "<!-- startquestion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "e79s8POBiRaF"
   },
   "outputs": [],
   "source": [
    "# Let's create some temporary weights and biases and test out our matrix operations before we build our model.\n",
    "# Create a weights parameter with 1 beta per column in X\n",
    "weights = ...\n",
    "# Create our bias parameter\n",
    "bias = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZW3sWkn49A9r",
    "outputId": "5333f884-1486-4ba1-de69-054ef2fe512a"
   },
   "outputs": [],
   "source": [
    "# Test out the operation we want to perform in the forward pass\n",
    "torch.matmul(X_train[:10], weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8bauLhD7N8V",
    "outputId": "52a0189a-718d-4715-e52f-7927a1f926df"
   },
   "outputs": [],
   "source": [
    "# FYI: @ does the same thing as matmul in this context\n",
    "X_train[:10]@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "mYRcEdGH8aN3"
   },
   "outputs": [],
   "source": [
    "# Sanity check: different implementations of our forward pass are the same\n",
    "assert (X_train@weights + bias == torch.matmul(X_train, weights) + bias).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "gNzWV8938wjO"
   },
   "outputs": [],
   "source": [
    "# Let's make our model\n",
    "class LinRegMulti(nn.Module):\n",
    "    def __init__(self, n_cols):\n",
    "        super().__init__()\n",
    "        self.n_cols = n_cols\n",
    "\n",
    "        self.weights = nn.Parameter(torch.rand(self.n_cols))\n",
    "        self.bias = nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X@self.weights.T + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "wEzQbpnR8hqO"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10000\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "MfWaPFjW9dfb"
   },
   "outputs": [],
   "source": [
    "lrm = LinRegMulti(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "UlRs5GF7-CQh"
   },
   "outputs": [],
   "source": [
    "# Instead of updating each parameter individually, let's make an update rule function.\n",
    "def gd_update_rule(parameters, lr):\n",
    "    parameters.data.sub_(parameters.grad * lr)\n",
    "    parameters.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "zzk5omp6-hEX"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GczinipS9hRz",
    "outputId": "83c0628b-f5f1-4f6c-b623-3ec9253b752e"
   },
   "outputs": [],
   "source": [
    "for i in range(N_EPOCHS):\n",
    "    yhat = lrm(X_train)\n",
    "    loss = mse(yhat, y_train)\n",
    "    loss.backward()\n",
    "    for p in lrm.parameters():\n",
    "        gd_update_rule(p, LR)\n",
    "    train_losses.append(loss.detach().numpy())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yhat = lrm(X_valid)\n",
    "        valid_loss = mse(yhat, y_valid)\n",
    "        valid_losses.append(valid_loss.numpy())\n",
    "\n",
    "    if i%(N_EPOCHS/10) == 0:\n",
    "        print(f\"Epoch {i} Train Loss: {loss:.04f}, Valid Loss: {valid_loss:.04f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RoVuHW7a9iau",
    "outputId": "96272874-9453-48f1-a456-20e7b7d38fc5"
   },
   "outputs": [],
   "source": [
    "EPOCHS_TO_SHOW = 2000\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.plot(train_losses[:EPOCHS_TO_SHOW], label='Train', linewidth=3, alpha=0.5)\n",
    "ax.plot(valid_losses[:EPOCHS_TO_SHOW], ls='--', label='Valid')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nVxNByIg-mas",
    "outputId": "0a209069-e90c-4e95-fd3a-24c9e8ff1f5c"
   },
   "outputs": [],
   "source": [
    "lrm.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYNLiUqy-yxx",
    "outputId": "e89524ff-7b68-492c-86da-2c3527f5f500"
   },
   "outputs": [],
   "source": [
    "lrm.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "59HrUTC5_YfS"
   },
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.rand((dim_in, dim_out)))\n",
    "        self.bias = nn.Parameter(torch.rand(dim_out))\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        return X@self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "glH-bIYp8JxC"
   },
   "outputs": [],
   "source": [
    "# Let's compare our Linear class with nn.Linear\n",
    "l1 = Linear(3, 5)\n",
    "l2 = nn.Linear(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNAS7jI99yGE",
    "outputId": "b9395ece-bc83-4342-a3c0-193e9719708a"
   },
   "outputs": [],
   "source": [
    "l2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgwbpaGw9wCT",
    "outputId": "4ecf2b4f-886e-450f-bbe2-cbe4a2e614e3"
   },
   "outputs": [],
   "source": [
    "l1.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHTeLzXP_Rzc",
    "outputId": "714e372d-2892-4bf8-b325-b9724d55f956"
   },
   "outputs": [],
   "source": [
    "# We need to make sure the weights have the same values.\n",
    "# If they don't, we won't be able to compare the output.\n",
    "# I'm not sure why the Linear layer's weights are transposed,\n",
    "# but we'll see it doesn't matter.\n",
    "l1.weights.data.copy_(l2.weight.T)\n",
    "l1.bias.data.copy_(l2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZsxCvsZ8c5R",
    "outputId": "b1cdb76c-3e18-4121-f9fe-519ff8eb06aa"
   },
   "outputs": [],
   "source": [
    "l1(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-l2dsw78oyO",
    "outputId": "99a060ed-1220-48cb-a6b7-08a8b3468273"
   },
   "outputs": [],
   "source": [
    "l2(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "OJK1Q27O_cdR"
   },
   "outputs": [],
   "source": [
    "assert (l1(X_train[:5]) == l2(X_train[:5])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRV8GSei7rt_",
    "outputId": "3dbe217f-9b4d-4749-a25b-51d5a1a22171"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "l1(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFOymbqq7wHO",
    "outputId": "f4bf3f54-e1dd-4073-ff7d-2ef371b96908"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    " l2(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "gE7TAbIG2tjD",
    "outputId": "5f79acc3-13ac-46af-f94d-c74be1ba13c5"
   },
   "outputs": [],
   "source": [
    "rng = torch.arange(-5, 5.01, 0.05)\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.plot(rng, F.relu(rng), label='ReLU')\n",
    "ax.plot(rng, torch.tanh(rng), label='tanh')\n",
    "ax.plot(rng, torch.sigmoid(rng), label='sigmoid')\n",
    "ax.plot(rng, F.leaky_relu(rng, negative_slope=0.01), ls='--', label='leaky ReLU')\n",
    "ax.set_ylim(-1.1, 1.1)\n",
    "ax.set_title('Common activation functions')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "E1zUoNys_9QL"
   },
   "outputs": [],
   "source": [
    "class MultiLayerRegressor(nn.Module):\n",
    "    def __init__(self, dim_in, hidden_dim):\n",
    "        super().__init__()\n",
    "        # self.first_layer = Linear(dim_in, hidden_dim)\n",
    "        self.first_layer = nn.Linear(dim_in, hidden_dim)\n",
    "        # self.second_layer = Linear(hidden_dim, 1)\n",
    "        self.second_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = self.first_layer(X)\n",
    "        # x = relu(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.second_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mc0dA6sxIFoe"
   },
   "outputs": [],
   "source": [
    "def multilayer_regressor(in_dim, hidden_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "FKLZZbRBAaBD"
   },
   "outputs": [],
   "source": [
    "# mlr = MultiLayerRegressor(3, 4)\n",
    "mlr = multilayer_regressor(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "0oaN64-iAwJ4"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "Yt1uzMjZBkWQ"
   },
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "N_EPOCHS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "CPuw46kBFdty"
   },
   "outputs": [],
   "source": [
    "# Notice that instead of iterating through our parameters and applying\n",
    "# an update rule, we're just using torch's built in SGD optimizer.\n",
    "opt = optim.SGD(mlr.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bf91-OmaAepE",
    "outputId": "74ffb4b5-bdfc-4356-d52e-c8370df95219"
   },
   "outputs": [],
   "source": [
    "for i in range(N_EPOCHS):\n",
    "    yhat = mlr(X_train).squeeze()\n",
    "    # Calculate the loss\n",
    "    loss = F.mse_loss(yhat, y_train)\n",
    "    # Calculate the gradients\n",
    "    loss.backward()\n",
    "    # Perform the update step\n",
    "    opt.step()\n",
    "    # Zero out the gradients\n",
    "    opt.zero_grad()\n",
    "    train_losses.append(loss.detach().numpy())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yhat = mlr(X_valid).squeeze()\n",
    "        valid_loss = F.mse_loss(yhat, y_valid)\n",
    "        valid_losses.append(loss.numpy())\n",
    "\n",
    "    if i%(N_EPOCHS/10) == 0:\n",
    "        print(f\"Epoch {i} Train loss: {loss:.04f}, Valid loss: {valid_loss:.04f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1FImQTDQA278",
    "outputId": "681d8b42-b1b1-4d1c-bdd9-5f2ffa6437a6"
   },
   "outputs": [],
   "source": [
    "idx=10000\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.plot(train_losses[:idx], label='Train', linewidth=3, alpha=0.5)\n",
    "ax.plot(valid_losses[:idx], ls='--', label='Valid')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "SXkBM19al6Ge"
   },
   "outputs": [],
   "source": [
    "# Modify the code above to complete the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "q-WNsgD3UZlj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_Our_first_neural_network_linear_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cndl",
   "language": "python",
   "name": "cndl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
