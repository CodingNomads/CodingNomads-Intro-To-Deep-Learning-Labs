{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XS2yv4IZTEj2"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -Uqqq spacy datasets tokenizers plotly\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fBiB_cCsVCMW"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tokenizers.models import WordLevel, BPE, WordPiece\n",
    "import tokenizers\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GxcBYVjJutrF"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtMyJfnBY3tf",
    "outputId": "42a7db07-798a-405e-f72c-b8fa34dd67e2"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"SetFit/emotion\", split='train')\n",
    "class_names = 'anger fear joy love sadness surprise'.split()\n",
    "class_lookup = {i:c for i, c in enumerate(class_names)}\n",
    "class_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2zxg1Y-NWlEf"
   },
   "outputs": [],
   "source": [
    "sentence1 = 'The quick brown fox jumped over the lazy dog'\n",
    "sentence2 = 'Deep learning is fun!'\n",
    "sentence3 = 'deep learning is hard.'\n",
    "\n",
    "sentences = [sentence1, sentence2, sentence3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "yIY_undPX3cT",
    "outputId": "b3c47337-2286-4d64-ee80-a45e744b2b5e"
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "sentences_cv = cv.fit_transform(sentences)\n",
    "sentences_cv = pd.DataFrame(sentences_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "\n",
    "sentences_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yW-2SlVz8EuO"
   },
   "outputs": [],
   "source": [
    "from tokenizers import normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "h6PObR-M8GsE"
   },
   "outputs": [],
   "source": [
    "normalizer = normalizers.Sequence([\n",
    "    normalizers.NFD(),\n",
    "    normalizers.Lowercase(),\n",
    "    normalizers.StripAccents()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7xTfIfN_9flm",
    "outputId": "39e08786-97ec-4a5f-d6d2-b99ccfcd5032"
   },
   "outputs": [],
   "source": [
    "normalizer.normalize_str('Höw aRę ŸõŪ dÔįñg?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGKqKqQxIYXD",
    "outputId": "efc2a4bd-3fda-46a7-c8a9-b05a802b89e2"
   },
   "outputs": [],
   "source": [
    "normalized_senences = [normalizer.normalize_str(s) for s in sentences]\n",
    "normalized_senences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEq_gHFkTM0x",
    "outputId": "bcd538de-c48f-472e-9ca3-b08f1817b124"
   },
   "outputs": [],
   "source": [
    "for s in normalized_senences:\n",
    "    print(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "AbRtX6dj2Dzb"
   },
   "outputs": [],
   "source": [
    "from tokenizers import pre_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5aae-P1j3QQ6",
    "outputId": "23cc26e9-2b26-4718-9701-035e20ba5c68"
   },
   "outputs": [],
   "source": [
    "pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "# split our normalized_sentences\n",
    "split_sentences = [pre_tokenizer.pre_tokenize_str(s) for s in normalized_senences]\n",
    "split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zQOSiUQe3mIw"
   },
   "outputs": [],
   "source": [
    "UNK_TOKEN = '[UNK]'\n",
    "PAD_TOKEN = '[PAD]'\n",
    "\n",
    "tokenizer = tokenizers.Tokenizer(model=tokenizers.models.WordLevel(unk_token=UNK_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "DA2-3Di74FwY"
   },
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizer\n",
    "tokenizer.pre_tokenizer = pre_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "JuvdbMlOxioh"
   },
   "outputs": [],
   "source": [
    "trainer = tokenizers.trainers.WordLevelTrainer(vocab_size=30000, min_frequency=10, show_progress=True, special_tokens=[PAD_TOKEN, UNK_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "K-3AgYbVx03a",
    "outputId": "868f2063-b1f4-46c8-b228-5dac56e1ba0e"
   },
   "outputs": [],
   "source": [
    "def document_iterator(ds):\n",
    "    for item in ds:\n",
    "        yield item['text']\n",
    "\n",
    "one_document = next(document_iterator(dataset))\n",
    "one_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOGiGV17xvE2",
    "outputId": "b349c990-641b-4b45-a7c7-e722eae53449"
   },
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(document_iterator(dataset), trainer)\n",
    "\n",
    "print(f\"\"\"\n",
    "Our tokenizer contains {tokenizer.get_vocab_size()} unique tokens.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3I2wlQ7KHGZ3",
    "outputId": "d2819385-e7b5-455b-d88b-af8eeeef8b0f"
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f'ID: {i}, token: {tokenizer.id_to_token(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "DHT736JByPOO",
    "outputId": "a5d4c07f-13cb-4d52-9fb2-fbba9f11ea14"
   },
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(one_document)\n",
    "pd.DataFrame(zip(encoded.tokens, encoded.ids), columns=['token', 'id']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "bZR-TsDIJFIg"
   },
   "outputs": [],
   "source": [
    "tokenizer.enable_padding(pad_id=tokenizer.token_to_id(PAD_TOKEN), pad_token=PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "9qDIfB7uPEvC"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    texts = [i['text'] for i in batch]\n",
    "    encoded = tokenizer.encode_batch(texts)\n",
    "    ids = [t.ids for t in encoded]\n",
    "    labels = [i['label'] for i in batch]\n",
    "    return torch.tensor(ids), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "K2tBsJKmMSoY"
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset, batch_size=8, collate_fn=collate_fn)\n",
    "encoded_texts, labels = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CN50tTGKMi1h",
    "outputId": "03786180-69ed-4728-cb32-cb573fed844f"
   },
   "outputs": [],
   "source": [
    "encoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "BD2GxduSpXd2"
   },
   "outputs": [],
   "source": [
    "# pass the dataset directly to the dataloader without a collate_fn\n",
    "dl = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "# tokenize each batch after it's loaded in the training loop\n",
    "for batch in dl:\n",
    "    encoded = tokenizer.encode_batch(batch['text'])\n",
    "    input_ids = torch.tensor([document.ids for document in encoded])\n",
    "    labels = batch['label']\n",
    "    # We'll also write the rest of the steps,\n",
    "    # although we're not actually training a model at the moment.\n",
    "    # logits = model(ids)\n",
    "    # loss = loss_fn(logits, labels)\n",
    "    # loss.backward()\n",
    "    # opt.step()\n",
    "    # opt.zero_grad()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QPFB20vhpdCA",
    "outputId": "e0d6354b-40ae-489d-ace4-e8ffcbc72c38"
   },
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qsdauq_8QCB_",
    "outputId": "a080071a-efa9-4210-e407-72b733d5aa2c"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode_batch(encoded_texts.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOc+iUAyDgfM5scRiRUjYtR",
   "include_colab_link": true,
   "name": "14_preparing_raw_text_for_nlp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
